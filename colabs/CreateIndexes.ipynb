{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CreateIndexes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "X_pR5rM-_NHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kaggle Dataset**"
      ],
      "metadata": {
        "id": "qKwjnvzMD5G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets"
      ],
      "metadata": {
        "id": "wmQ7EhYWD2jZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d53632c-5a51-4e29-8038-a6d7cb675cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.20-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.62.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2021.10.8)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (5.0.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od"
      ],
      "metadata": {
        "id": "S6Ctf8fCD8ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для следующей ячейки потребуются данные из Kaggle аккаунта:\n",
        "\n",
        "You Profile -> Account -> Create New API Token"
      ],
      "metadata": {
        "id": "nFjL1D9w8WPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/jessicali9530/stl10\")"
      ],
      "metadata": {
        "id": "e5cMBwZFEBQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1d5774-89c4-4618-993d-80fe434c9d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: lightlegends\n",
            "Your Kaggle Key: ··········\n",
            "Downloading stl10.zip to ./stl10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.88G/1.88G [00:56<00:00, 35.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Work with SearchModel"
      ],
      "metadata": {
        "id": "zs1P0jKsfXFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ruclip==0.0.1 > /dev/null\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "# Для colab нижние install не нужны, могут пригодиться для сервера, только нужно ставить torchvision для cpu, а не как снизу для cuda\n",
        "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1GzMZZprsrN",
        "outputId": "b1b49df3-7d71-4ed3-d512-de2eb486a67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ezrs9w4j\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-ezrs9w4j\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.19.5)\n",
            "Building wheels for collected packages: clip, ftfy\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369221 sha256=29518b531e4aace9fa65994377eadc58f6217a16435666eba85638f92ebb1aa1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xcr6lom_/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=72a14e115cdc3c8459b18e3200edfde8d7d7038482118eefd9f8339a775f1bc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built clip ftfy\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# from faiss import Indexer\n",
        "\n",
        "\n",
        "class DummyIndexer():\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Creates an empty index object\n",
        "        \"\"\"\n",
        "        self.index = None\n",
        "\n",
        "    def add(self, embs: np.ndarray):\n",
        "        \"\"\"\n",
        "        Adds new embeddings embs in empty or existing index\n",
        "        :param embs:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if self.index is None:\n",
        "            self.index = embs\n",
        "        else:\n",
        "            self.index = np.append(self.index, embs, axis=0)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Not sure if this one is necessary here, left for compatibility with abstract class Indexer\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def find(self, query: np.ndarray, topn: int) -> (np.ndarray, np.ndarray):\n",
        "        \"\"\"\n",
        "        Returns topn entries closest to the query vector\n",
        "        :param query:\n",
        "        :param topn:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        similarities = (self.index @ query.squeeze())\n",
        "        best_photo_idx = (-similarities).argsort()\n",
        "        D, I = similarities[best_photo_idx[:topn]], best_photo_idx[:topn]\n",
        "        return D, I\n",
        "\n",
        "    def save(self, file: str):\n",
        "        \"\"\"\n",
        "        Saves data to npy file\n",
        "        :param file:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        np.save(file, self.index)\n",
        "\n",
        "    def load(self, file: str):\n",
        "        \"\"\"\n",
        "        Loads data from npy file\n",
        "        :param file:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self.index = np.load(file)"
      ],
      "metadata": {
        "id": "4_tc1BeztwP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Created on 2022 Jan 28 14:09 \n",
        "@author: keller\n",
        "\"\"\"\n",
        "import abc\n",
        "\n",
        "import torch\n",
        "import ruclip\n",
        "import clip\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from numbers import Number\n",
        "from typing import List\n",
        "\n",
        "class Embedder(abc.ABC):\n",
        "    @abc.abstractmethod\n",
        "    def encode_text(self, text):\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def encode_imgs(self, imgs):\n",
        "        pass\n",
        "\n",
        "    def cos(self, emb1: np.ndarray, emb2: np.ndarray) -> Number:\n",
        "        \"\"\"\n",
        "        Returns cos similarity between two embeddings\n",
        "        :param emb1: 1D tensor\n",
        "        :param emb2: 1D tensor\n",
        "        :return: cos similarity (Number)\n",
        "        \"\"\"\n",
        "        emb1, emb2 = emb1.squeeze(), emb2.squeeze() # convert (1, N) arrays to (N,)\n",
        "        return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "\n",
        "\n",
        "class EmbedderRuCLIP(Embedder):\n",
        "    def __init__(self, ruclip_model_name='ruclip-vit-base-patch32-384',\n",
        "             device='cpu', templates = ['{}', 'это {}', 'на картинке {}']):\n",
        "        \"\"\"\n",
        "        :param ruclip_model_name:\n",
        "        :param device:\n",
        "        :param templates:\n",
        "        \"\"\"\n",
        "        clip, processor = ruclip.load(ruclip_model_name)\n",
        "        self.predictor = ruclip.Predictor(clip, processor, device, bs=8, templates=templates)\n",
        "\n",
        "    def _tonumpy(self, tensor: torch.Tensor) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Detaches tensor from GPU and converts it to numpy array\n",
        "        :return: numpy array\n",
        "        \"\"\"\n",
        "        return tensor.cpu().detach().numpy()\n",
        "\n",
        "    def encode_text(self, text: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns text latent of the text input\n",
        "        :param text:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        classes = [text, ]\n",
        "        with torch.no_grad():\n",
        "            text_latent = self.predictor.get_text_latents(classes)\n",
        "        return self._tonumpy(text_latent)\n",
        "\n",
        "    def encode_imgs(self, pil_imgs: List[Image.Image]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns image latents of a image batch\n",
        "        :param pil_imgs: list of PIL images\n",
        "        :return img_latents: numpy array of img latents\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            img_latents = self.predictor.get_image_latents(pil_imgs)\n",
        "        return self._tonumpy(img_latents)\n",
        "\n",
        "class EmbedderCLIP(Embedder):\n",
        "    def __init__(self, clip_model_name='ViT-B/32', device='cpu'):\n",
        "        \"\"\"\n",
        "        :param clip_model_name:\n",
        "        :param device:\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.predictor, self.preprocess = clip.load(clip_model_name, device=device)\n",
        "\n",
        "    def _tonumpy(self, tensor: torch.Tensor) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Detaches tensor from GPU and converts it to numpy array\n",
        "        :return: numpy array\n",
        "        \"\"\"\n",
        "        return tensor.cpu().detach().numpy()\n",
        "\n",
        "    def encode_text(self, text: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns text latent of the text input\n",
        "        :param text:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "          # Encode it to a feature vector using CLIP\n",
        "          text_latent = self.predictor.encode_text(clip.tokenize(text).to(self.device))\n",
        "          text_latent /= text_latent.norm(dim=-1, keepdim=True)\n",
        "          \n",
        "        return self._tonumpy(text_latent)\n",
        "\n",
        "    def encode_imgs(self, pil_imgs: List[Image.Image]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns image latents of a image batch\n",
        "        :param pil_imgs: list of PIL images\n",
        "        :return img_latents: numpy array of img latents\n",
        "        \"\"\"\n",
        "\n",
        "        # Preprocess all photos\n",
        "        photos_preprocessed = torch.stack([self.preprocess(photo) for photo in pil_imgs]).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # Encode the photos batch to compute the feature vectors and normalize them\n",
        "          img_latents = self.predictor.encode_image(photos_preprocessed)\n",
        "          img_latents /= img_latents.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return self._tonumpy(img_latents)"
      ],
      "metadata": {
        "id": "32X9OSv1pxZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from PIL import Image\n",
        "from typing import List\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class SearchModel():\n",
        "    def __init__(self, embedder, indexer):\n",
        "        self.embedder = embedder\n",
        "        self.indexer = indexer\n",
        "        self.images_dir = None\n",
        "        self.imgs_path = None\n",
        "        self.features_path = None\n",
        "\n",
        "    def load_imgs(self, path: str, prefix: str):\n",
        "        \"\"\"\n",
        "        Returns a list of names images in a given path\n",
        "        :param path:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self.images_dir = path\n",
        "        photos_path = Path(self.images_dir)\n",
        "        general_features_dir = str(photos_path.parents[0]) + '/features'\n",
        "        features_dir = general_features_dir + '/' + prefix\n",
        "        self.features_path = Path(features_dir)\n",
        "        self.imgs_path = list(photos_path.glob(\"*.*\"))\n",
        "        \n",
        "        if not os.path.exists(general_features_dir):\n",
        "          os.mkdir(general_features_dir)\n",
        "        \n",
        "        if not os.path.exists(features_dir):\n",
        "          os.mkdir(features_dir)\n",
        "        \n",
        "        if len(os.listdir(features_dir)) >= 2:\n",
        "          self.imgs_path = list(pd.read_csv(f\"{self.features_path}/photo_ids.csv\")['photo_id'])\n",
        "\n",
        "    def load_img_urls(self):\n",
        "        \"\"\"\n",
        "        In case we want to load imgs from a list of url\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def save_embs(self, batch_size=512) -> None:\n",
        "        \"\"\"\n",
        "        Extracts image embeddings from embedder and adds them to indexer\n",
        "        :param pil_imgs:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        if len(os.listdir(self.features_path)) >= 2:\n",
        "          os.remove(str(self.features_path) + '/photo_ids.csv')\n",
        "          os.remove(str(self.features_path) + '/features.npy')\n",
        "          self.imgs_path = list(Path(self.images_dir).glob(\"*.*\"))\n",
        "        \n",
        "        if not len(self.imgs_path) >= 512:\n",
        "          batch_size = len(self.imgs_path)\n",
        "\n",
        "        # Compute how many batches are needed\n",
        "        batches = math.ceil(len(self.imgs_path) / batch_size)\n",
        "\n",
        "        # Process each batch\n",
        "        for i in range(batches):\n",
        "          print(f\"Processing batch {i+1}/{batches}\")\n",
        "\n",
        "          batch_ids_path = self.features_path / f\"{i:010d}.csv\"\n",
        "          batch_features_path = self.features_path / f\"{i:010d}.npy\"\n",
        "    \n",
        "          # Only do the processing if the batch wasn't processed yet\n",
        "          if not batch_features_path.exists():\n",
        "            try:\n",
        "              # Select the photos for the current batch\n",
        "              batch_files = self.imgs_path[i*batch_size : min(len(self.imgs_path), (i+1)*batch_size)]\n",
        "              pil_batch = [Image.open(photo_file) for photo_file in batch_files]\n",
        "\n",
        "              # Compute the features and save to a numpy file\n",
        "              batch_features = self.embedder.encode_imgs(pil_batch)\n",
        "              np.save(batch_features_path, batch_features)\n",
        "\n",
        "              # Save the photo IDs to a CSV file\n",
        "              photo_ids = [photo_file for photo_file in batch_files]\n",
        "              photo_ids_data = pd.DataFrame(photo_ids, columns=['photo_id'])\n",
        "              photo_ids_data.to_csv(batch_ids_path, index=False)\n",
        "            except:\n",
        "              # Catch problems with the processing to make the process more robust\n",
        "              print(f'Problem with batch {i}')\n",
        "\n",
        "        # Load all numpy files\n",
        "        features_list = [np.load(features_file) for features_file in sorted(self.features_path.glob(\"*.npy\"))]\n",
        "\n",
        "        # Concatenate the features and store in a merged file\n",
        "        features = np.concatenate(features_list)\n",
        "        np.save(self.features_path / \"features.npy\", features)\n",
        "\n",
        "        # Load all the photo IDs\n",
        "        photo_ids = pd.concat([pd.read_csv(ids_file) for ids_file in sorted(self.features_path.glob(\"*.csv\"))])\n",
        "        photo_ids.to_csv(self.features_path / \"photo_ids.csv\", index=False)\n",
        "        \n",
        "        for file in glob.glob('{}/0*.*'.format(self.features_path)):\n",
        "          os.remove(file)\n",
        "        \n",
        "        self.indexer.load(str(self.features_path) + '/features.npy')\n",
        "    \n",
        "    def get_k_imgs(self, emb: np.ndarray, k: int):\n",
        "        \"\"\"\n",
        "        Returns k indices of nearest image embeddings and respective distances for a given embedding emb\n",
        "        :param emb:\n",
        "        :param k:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        distances, indices = self.indexer.find(emb, k)\n",
        "        return distances, np.array(self.imgs_path)[indices]"
      ],
      "metadata": {
        "id": "Jezr0YaofYGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Строим индексы"
      ],
      "metadata": {
        "id": "wId_NXOu8A5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В функцию load_imgs подайте путь до данных: \"stl10\" в нашем случае название датасета, \"train_images\" где хранятся изображения. \n",
        "\n",
        "Для своего индекса сохраняйте такую же структуру: {Название датасета}/{Где хранятся изображения}.\n",
        "\n",
        "Когда код выполнится: нужно скачать получение признаки, находятся они по пути: {Название датасета}/{features}.\n",
        "\n",
        "Сами изображения тоже должны находится на сервере, в папке с названием {Название датасета}, общая папка для всех - это \"indexes\". Пример правильной архитектуры находится на сервере по пути \"/home/comptech/indexes/trip\"."
      ],
      "metadata": {
        "id": "yWqUvipN8nB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model = SearchModel(EmbedderCLIP(device='cuda'), DummyIndexer())\n",
        "ruclip_model = SearchModel(EmbedderRuCLIP(device='cuda'), DummyIndexer())\n",
        "\n",
        "clip_model.load_imgs('/content/stl10/train_images','CLIP')\n",
        "clip_model.save_embs()\n",
        "ruclip_model.load_imgs('/content/stl10/train_images','RuCLIP')\n",
        "ruclip_model.save_embs()"
      ],
      "metadata": {
        "id": "l35FHrlx8Eq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = clip_model.embedder.encode_text(text=\"Small monkey\")\n",
        "clip_model.get_k_imgs(query, 10)"
      ],
      "metadata": {
        "id": "1of8IKOx-a8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = ruclip_model.embedder.encode_text(text=\"Обезьяна играет с мячиком\")\n",
        "ruclip_model.get_k_imgs(query, 10)"
      ],
      "metadata": {
        "id": "4Ds_Us1X-YAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Самое главное действие !**"
      ],
      "metadata": {
        "id": "jLivI4Up-uTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поменяйте выражение trip на то, как будет называться ваш датасет: '/home/comptech/indexes/trip/images/'.\n",
        "\n",
        "Допустимо: name; name_prefix.\n",
        "\n",
        "Делать это в коде, а не в текстовом блоке."
      ],
      "metadata": {
        "id": "v5LRBPUl_q-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_true_path(data_in_list, name_file):\n",
        "  photo_id_list = []\n",
        "\n",
        "  for name in data_in_list:\n",
        "    check = '/home/comptech/indexes/trip/images/' + [name.split('/')[len(name.split('/'))-1]][0]\n",
        "    photo_id_list.append(check)\n",
        "\n",
        "  photo_ids_data = pd.DataFrame(photo_id_list, columns=['photo_id'])\n",
        "  photo_ids_data.to_csv(name_file, index=False)"
      ],
      "metadata": {
        "id": "LvNvyOuW_KgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data_clip = pd.read_csv('напишите_путь_до.csv')\n",
        "data_clip_in_list = data_clip['photo_id'].to_list()\n",
        "\n",
        "data_ruclip = pd.read_csv('напишите_путь_до.csv')\n",
        "data_ruclip_in_list = data_ruclip['photo_id'].to_list()\n",
        "\n",
        "generate_true_path(data_clip_in_list, \"photo_ids_clip.csv\")\n",
        "generate_true_path(data_ruclip_in_list, \"photo_ids_ruclip.csv\")"
      ],
      "metadata": {
        "id": "rHs4Dp3P-1_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Не забудьте скачать посчитанные сверху csv**"
      ],
      "metadata": {
        "id": "anX94btXC-ar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Осталось сохранить посчитанные признаки и сам датасет на свой гугл диск, а после открыть к нему доступ и скачать на сервер с помощью консоли или же самому закинуть его на сервер через Xftp"
      ],
      "metadata": {
        "id": "SW3uoRKZB5Z1"
      }
    }
  ]
}