{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SearchModel.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JmeaibAxeYZH"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download Dataset"
      ],
      "metadata": {
        "id": "X_pR5rM-_NHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kaggle Dataset**"
      ],
      "metadata": {
        "id": "qKwjnvzMD5G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets"
      ],
      "metadata": {
        "id": "wmQ7EhYWD2jZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0032d1b0-6fdf-4990-84c2-d19acff021b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.20-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (5.0.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2021.10.8)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od"
      ],
      "metadata": {
        "id": "S6Ctf8fCD8ir"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для следующей ячейки потребуются данные из Kaggle аккаунта:\n",
        "\n",
        "You Profile -> Account -> Create New API Token"
      ],
      "metadata": {
        "id": "nFjL1D9w8WPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "od.download(\"https://www.kaggle.com/jessicali9530/stl10\")"
      ],
      "metadata": {
        "id": "e5cMBwZFEBQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0765dcd1-389c-4366-9a3c-d4fab42454a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: lightlegends\n",
            "Your Kaggle Key: ··········\n",
            "Downloading stl10.zip to ./stl10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.88G/1.88G [00:18<00:00, 110MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Work with SearchModel"
      ],
      "metadata": {
        "id": "zs1P0jKsfXFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ruclip==0.0.1 > /dev/null\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "# Для colab нижние install не нужны, могут пригодиться для сервера, только нужно ставить torchvision для cpu, а не как снизу для cuda\n",
        "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1GzMZZprsrN",
        "outputId": "955b8783-7296-4051-f7a3-3db6dc25e526"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-mqskej22\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-mqskej22\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.62.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.19.5)\n",
            "Building wheels for collected packages: clip, ftfy\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369221 sha256=5d0cf59acbf78785f437f5a0c8f7e5597265b8995cd5c89cef351d924d174e1d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g7dyceh2/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=4d49a50e6671ece0ec49eee5387acdf547a1124418f58707302a03e3b9535cef\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built clip ftfy\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# from faiss import Indexer\n",
        "\n",
        "\n",
        "class DummyIndexer():\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Creates an empty index object\n",
        "        \"\"\"\n",
        "        self.index = None\n",
        "\n",
        "    def add(self, embs: np.ndarray):\n",
        "        \"\"\"\n",
        "        Adds new embeddings embs in empty or existing index\n",
        "        :param embs:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        if self.index is None:\n",
        "            self.index = embs\n",
        "        else:\n",
        "            self.index = np.append(self.index, embs, axis=0)\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Not sure if this one is necessary here, left for compatibility with abstract class Indexer\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def find(self, query: np.ndarray, topn: int) -> (np.ndarray, np.ndarray):\n",
        "        \"\"\"\n",
        "        Returns topn entries closest to the query vector\n",
        "        :param query:\n",
        "        :param topn:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        similarities = (self.index @ query.squeeze())\n",
        "        best_photo_idx = (-similarities).argsort()\n",
        "        D, I = similarities[best_photo_idx[:topn]], best_photo_idx[:topn]\n",
        "        return D, I\n",
        "\n",
        "    def save(self, file: str):\n",
        "        \"\"\"\n",
        "        Saves data to npy file\n",
        "        :param file:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        np.save(file, self.index)\n",
        "\n",
        "    def load(self, file: str):\n",
        "        \"\"\"\n",
        "        Loads data from npy file\n",
        "        :param file:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self.index = np.load(file)"
      ],
      "metadata": {
        "id": "4_tc1BeztwP9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Created on 2022 Jan 28 14:09 \n",
        "@author: keller\n",
        "\"\"\"\n",
        "import abc\n",
        "\n",
        "import torch\n",
        "import ruclip\n",
        "import clip\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from numbers import Number\n",
        "from typing import List\n",
        "\n",
        "class Embedder(abc.ABC):\n",
        "    @abc.abstractmethod\n",
        "    def encode_text(self, text):\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def encode_imgs(self, imgs):\n",
        "        pass\n",
        "\n",
        "    def cos(self, emb1: np.ndarray, emb2: np.ndarray) -> Number:\n",
        "        \"\"\"\n",
        "        Returns cos similarity between two embeddings\n",
        "        :param emb1: 1D tensor\n",
        "        :param emb2: 1D tensor\n",
        "        :return: cos similarity (Number)\n",
        "        \"\"\"\n",
        "        emb1, emb2 = emb1.squeeze(), emb2.squeeze() # convert (1, N) arrays to (N,)\n",
        "        return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "\n",
        "\n",
        "class EmbedderRuCLIP(Embedder):\n",
        "    def __init__(self, ruclip_model_name='ruclip-vit-base-patch32-384',\n",
        "             device='cpu', templates = ['{}', 'это {}', 'на картинке {}']):\n",
        "        \"\"\"\n",
        "        :param ruclip_model_name:\n",
        "        :param device:\n",
        "        :param templates:\n",
        "        \"\"\"\n",
        "        clip, processor = ruclip.load(ruclip_model_name)\n",
        "        self.predictor = ruclip.Predictor(clip, processor, device, bs=8, templates=templates)\n",
        "\n",
        "    def _tonumpy(self, tensor: torch.Tensor) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Detaches tensor from GPU and converts it to numpy array\n",
        "        :return: numpy array\n",
        "        \"\"\"\n",
        "        return tensor.cpu().detach().numpy()\n",
        "\n",
        "    def encode_text(self, text: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns text latent of the text input\n",
        "        :param text:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        classes = [text, ]\n",
        "        with torch.no_grad():\n",
        "            text_latent = self.predictor.get_text_latents(classes)\n",
        "        return self._tonumpy(text_latent)\n",
        "\n",
        "    def encode_imgs(self, pil_imgs: List[Image.Image]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns image latents of a image batch\n",
        "        :param pil_imgs: list of PIL images\n",
        "        :return img_latents: numpy array of img latents\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            img_latents = self.predictor.get_image_latents(pil_imgs)\n",
        "        return self._tonumpy(img_latents)\n",
        "\n",
        "class EmbedderCLIP(Embedder):\n",
        "    def __init__(self, clip_model_name='ViT-B/32', device='cpu'):\n",
        "        \"\"\"\n",
        "        :param clip_model_name:\n",
        "        :param device:\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.predictor, self.preprocess = clip.load(clip_model_name, device=device)\n",
        "\n",
        "    def _tonumpy(self, tensor: torch.Tensor) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Detaches tensor from GPU and converts it to numpy array\n",
        "        :return: numpy array\n",
        "        \"\"\"\n",
        "        return tensor.cpu().detach().numpy()\n",
        "\n",
        "    def encode_text(self, text: str) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns text latent of the text input\n",
        "        :param text:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "          # Encode it to a feature vector using CLIP\n",
        "          text_latent = self.predictor.encode_text(clip.tokenize(text).to(self.device))\n",
        "          text_latent /= text_latent.norm(dim=-1, keepdim=True)\n",
        "          \n",
        "        return self._tonumpy(text_latent)\n",
        "\n",
        "    def encode_imgs(self, pil_imgs: List[Image.Image]) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns image latents of a image batch\n",
        "        :param pil_imgs: list of PIL images\n",
        "        :return img_latents: numpy array of img latents\n",
        "        \"\"\"\n",
        "\n",
        "        # Preprocess all photos\n",
        "        photos_preprocessed = torch.stack([self.preprocess(photo) for photo in pil_imgs]).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # Encode the photos batch to compute the feature vectors and normalize them\n",
        "          img_latents = self.predictor.encode_image(photos_preprocessed)\n",
        "          img_latents /= img_latents.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return self._tonumpy(img_latents)"
      ],
      "metadata": {
        "id": "32X9OSv1pxZy"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from PIL import Image\n",
        "from typing import List\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "class SearchModel():\n",
        "    def __init__(self, embedder, indexer):\n",
        "        self.embedder = embedder\n",
        "        self.indexer = indexer\n",
        "        self.images_dir = None\n",
        "        self.imgs_path = None\n",
        "        self.features_path = None\n",
        "\n",
        "    def load_imgs(self, path: str, prefix: str):\n",
        "        \"\"\"\n",
        "        Returns a list of names images in a given path\n",
        "        :param path:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self.images_dir = path\n",
        "        photos_path = Path(self.images_dir)\n",
        "        general_features_dir = str(photos_path.parents[0]) + '/features'\n",
        "        features_dir = general_features_dir + '/' + prefix\n",
        "        self.features_path = Path(features_dir)\n",
        "        self.imgs_path = list(photos_path.glob(\"*.*\"))\n",
        "        \n",
        "        if not os.path.exists(general_features_dir):\n",
        "          os.mkdir(general_features_dir)\n",
        "        \n",
        "        if not os.path.exists(features_dir):\n",
        "          os.mkdir(features_dir)\n",
        "        \n",
        "        if len(os.listdir(features_dir)) >= 2:\n",
        "          self.imgs_path = list(pd.read_csv(f\"{self.features_path}/photo_ids.csv\")['photo_id'])\n",
        "\n",
        "    def load_img_urls(self):\n",
        "        \"\"\"\n",
        "        In case we want to load imgs from a list of url\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def add_photo_path(self, name):\n",
        "        return f'{self.images_dir}/{name}.png'\n",
        "\n",
        "    def save_embs(self) -> None:\n",
        "        \"\"\"\n",
        "        Extracts image embeddings from embedder and adds them to indexer\n",
        "        :param pil_imgs:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        if len(os.listdir(self.features_path)) >= 2:\n",
        "          os.remove(str(self.features_path) + '/photo_ids.csv')\n",
        "          os.remove(str(self.features_path) + '/features.npy')\n",
        "          self.imgs_path = list(Path(self.images_dir).glob(\"*.*\"))\n",
        "        \n",
        "        if(len(self.imgs_path) >= 512):\n",
        "          batch_size = 512\n",
        "        else:\n",
        "          batch_size = len(self.imgs_path)\n",
        "\n",
        "        # Compute how many batches are needed\n",
        "        batches = math.ceil(len(self.imgs_path) / batch_size)\n",
        "\n",
        "        # Process each batch\n",
        "        for i in range(batches):\n",
        "          print(f\"Processing batch {i+1}/{batches}\")\n",
        "\n",
        "          batch_ids_path = self.features_path / f\"{i:010d}.csv\"\n",
        "          batch_features_path = self.features_path / f\"{i:010d}.npy\"\n",
        "    \n",
        "          # Only do the processing if the batch wasn't processed yet\n",
        "          if not batch_features_path.exists():\n",
        "            try:\n",
        "              # Select the photos for the current batch\n",
        "              batch_files = self.imgs_path[i*batch_size : min(len(self.imgs_path), (i+1)*batch_size)]\n",
        "              pil_batch = [Image.open(photo_file) for photo_file in batch_files]\n",
        "\n",
        "              # Compute the features and save to a numpy file\n",
        "              batch_features = self.embedder.encode_imgs(pil_batch)\n",
        "              np.save(batch_features_path, batch_features)\n",
        "\n",
        "              # Save the photo IDs to a CSV file\n",
        "              photo_ids = [photo_file.name.split(\".\")[0] for photo_file in batch_files]\n",
        "              photo_ids_data = pd.DataFrame(photo_ids, columns=['photo_id'])\n",
        "              photo_ids_data.to_csv(batch_ids_path, index=False)\n",
        "            except:\n",
        "              # Catch problems with the processing to make the process more robust\n",
        "              print(f'Problem with batch {i}')\n",
        "\n",
        "        # Load all numpy files\n",
        "        features_list = [np.load(features_file) for features_file in sorted(self.features_path.glob(\"*.npy\"))]\n",
        "\n",
        "        # Concatenate the features and store in a merged file\n",
        "        features = np.concatenate(features_list)\n",
        "        np.save(self.features_path / \"features.npy\", features)\n",
        "\n",
        "        # Load all the photo IDs\n",
        "        photo_ids = pd.concat([pd.read_csv(ids_file) for ids_file in sorted(self.features_path.glob(\"*.csv\"))])\n",
        "        photo_ids = photo_ids[\"photo_id\"].apply(self.add_photo_path)\n",
        "        photo_ids.to_csv(self.features_path / \"photo_ids.csv\", index=False)\n",
        "        \n",
        "        for file in glob.glob('{}/0*.*'.format(self.features_path)):\n",
        "          os.remove(file)\n",
        "        \n",
        "        self.indexer.load(str(self.features_path) + '/features.npy')\n",
        "    \n",
        "    def get_k_imgs(self, emb: np.ndarray, k: int):\n",
        "        \"\"\"\n",
        "        Returns k indices of nearest image embeddings and respective distances for a given embedding emb\n",
        "        :param emb:\n",
        "        :param k:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        distances, indices = self.indexer.find(emb, k)\n",
        "        return distances, np.array(self.imgs_path)[indices]"
      ],
      "metadata": {
        "id": "Jezr0YaofYGP"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests for CLIP"
      ],
      "metadata": {
        "id": "vXjhlhkJdLB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для создание индексов в Colab"
      ],
      "metadata": {
        "id": "cWOL12nBdTIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = SearchModel(EmbedderCLIP(device='cuda'), DummyIndexer())"
      ],
      "metadata": {
        "id": "uIdOKmcyHpdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "960d7a51-bcbe-4b50-aef4-1de1dcc53fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 158MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prefix зависит от того, какую модель вы подали в SearchModel\n",
        "\n",
        "Как для произвольной папки с изображениями запустить обработку?\n",
        "\n",
        "test_model.load_imgs('/content/{Название датасета}/{Название папки с картинками}', 'Модель, которая будет обрабатывать')\n",
        "\n",
        "test_model.save_embs()\n",
        "\n",
        "Если нету название датасета ('/content/{Название папки с картинками}'), то папка \"features\" создаться по пути '/content/features'"
      ],
      "metadata": {
        "id": "ypRBScUXdcWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model.load_imgs('/content/stl10/train_images','CLIP')\n",
        "test_model.save_embs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq9Fbt91HuW4",
        "outputId": "27f3aa4f-bbfa-47dd-c075-5dc367619bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 1/10\n",
            "Processing batch 2/10\n",
            "Processing batch 3/10\n",
            "Processing batch 4/10\n",
            "Processing batch 5/10\n",
            "Processing batch 6/10\n",
            "Processing batch 7/10\n",
            "Processing batch 8/10\n",
            "Processing batch 9/10\n",
            "Processing batch 10/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = test_model.embedder.encode_imgs([Image.open('/content/monkey.png')])\n",
        "test_model.get_k_imgs(query, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph826OLuWqY4",
        "outputId": "6f4c5908-8d1f-423b-9312-a2a4b23ea40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.9907, 0.8516, 0.849 ], dtype=float16),\n",
              " array([PosixPath('/content/stl10/train_images/train_image_png_1815.png'),\n",
              "        PosixPath('/content/stl10/train_images/train_image_png_1240.png'),\n",
              "        PosixPath('/content/stl10/train_images/train_image_png_3014.png')],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = test_model.embedder.encode_text(text=\"Small monkey\")\n",
        "test_model.get_k_imgs(query, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWHuw5o_IlEy",
        "outputId": "9db43859-c765-49b4-af20-69fec0e090f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.3118, 0.3118, 0.3118], dtype=float16),\n",
              " array(['/content/stl10/train_images//content/stl10/train_images/train_image_png_1814.png.png',\n",
              "        PosixPath('/content/stl10/train_images/train_image_png_1814.png'),\n",
              "        PosixPath('/content/stl10/train_images/train_image_png_1814.png')],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для работы в Streamlit"
      ],
      "metadata": {
        "id": "_SCTGeO_dW0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прежде чем работать в Streamlit, нужно создать индекс. Сверху показано как это сделать."
      ],
      "metadata": {
        "id": "sNuMxUH7dezc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_streamlit = SearchModel(EmbedderCLIP(device='cpu'), DummyIndexer())"
      ],
      "metadata": {
        "id": "C8oHMYKSJG-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'/content/stl10/train_images' - Путь к самим изображениям\n",
        "\n",
        "'/content/stl10/features' - Путь, где находится csv и npy файл\n",
        "\n",
        "На сервере в папке датасета нужно будет создать папку \"features\" в ней \"CLIP\"/\"RuCLIP\" или скачивать всё с Colab и переносить в нужный путь\n",
        "\n",
        "Выбор prefix зависит от подаваемого текста или же от того, на какой модели пользователь хочет получить результат, в данный момент поддерживаются \"CLIP\" и \"RuCLIP\"\n"
      ],
      "metadata": {
        "id": "lA2FRDWeeTpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_streamlit.load_imgs('/content/stl10/train_images','CLIP')"
      ],
      "metadata": {
        "id": "CPNBnG-RJG-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_streamlit.indexer.load(str(test_streamlit.features_path) + '/features.npy')"
      ],
      "metadata": {
        "id": "DAQDhZM9JG-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = test_streamlit.embedder.encode_text(text=\"Small monkey\")\n",
        "test_streamlit.get_k_imgs(query, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5910fb-d776-492c-aaf3-b8c073d84def",
        "id": "_xJixlREJG-R"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.3116492 , 0.30705655, 0.3059756 ], dtype=float32),\n",
              " array(['/content/stl10/train_images/train_image_png_1814.png',\n",
              "        '/content/stl10/train_images/train_image_png_3279.png',\n",
              "        '/content/stl10/train_images/train_image_png_4511.png'],\n",
              "       dtype='<U52'))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test for RuCLIP"
      ],
      "metadata": {
        "id": "JmeaibAxeYZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для создание индексов в Colab"
      ],
      "metadata": {
        "id": "lGAkHZDWtPt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model = SearchModel(EmbedderRuCLIP(device='cuda'), DummyIndexer())"
      ],
      "metadata": {
        "id": "_n_-kvVNse9_"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prefix зависит от того, какую модель вы подали в SearchModel\n",
        "\n",
        "Как для произвольной папки с изображениями запустить обработку?\n",
        "\n",
        "test_model.load_imgs('/content/{Название датасета}/{Название папки с картинками}', 'Модель, которая будет обрабатывать')\n",
        "\n",
        "test_model.save_embs()\n",
        "\n",
        "Если нету название датасета ('/content/{Название папки с картинками}'), то папка \"features\" создаться по пути '/content/features'"
      ],
      "metadata": {
        "id": "hxJqMTX1efbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_model.load_imgs('/content/stl10/train_images','RuCLIP')\n",
        "test_model.save_embs()"
      ],
      "metadata": {
        "id": "3ehUwO7Wt5Oc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf5143a-460e-4de6-fc27-a36ccba12c34"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "512it [00:09, 51.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "512it [00:09, 53.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "512it [00:09, 52.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "512it [00:09, 52.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "512it [00:09, 52.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "512it [00:09, 52.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "512it [00:09, 52.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "512it [00:09, 52.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "512it [00:09, 52.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "392it [00:07, 52.78it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#query = test_model.embedder.encode_imgs([Image.open('/content/monkey.png')])\n",
        "query = test_model.embedder.encode_text(text=\"Обезьяна играет с мячиком\")\n",
        "test_model.get_k_imgs(query, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqLTpwdsWjT8",
        "outputId": "d7f1910c-1e90-4366-9b14-04cf1e15c165"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.53042364, 0.4278646 , 0.41783807, 0.3996673 , 0.3971902 ,\n",
              "        0.3928465 , 0.38977405, 0.385751  , 0.38466194, 0.38124365],\n",
              "       dtype=float32),\n",
              " array([PosixPath('/content/stl10/train_images/train_image_png_1815.png'),\n",
              "        PosixPath('/content/stl10/train_images/train_image_png_2566.png'),\n",
              "        PosixPath('/content/stl10/train_images/train_image_png_2900.png'),\n",
              "        PosixPath('/content/stl10/train_images/train_image_png_634.png'),\n",
              "        PosixPath('/content/stl10/train_images/train_image_png_3279.png'),\n",
              "        PosixPath('/content/stl10/train_images/train_image_png_2064.png'),\n",
              "        PosixPath('/content/stl10/train_images/train_image_png_205.png'),\n",
              "        PosixPath('/content/stl10/train_images/train_image_png_3541.png'),\n",
              "        PosixPath('/content/stl10/train_images/train_image_png_3367.png'),\n",
              "        PosixPath('/content/stl10/train_images/train_image_png_2909.png')],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для работы в Streamlit"
      ],
      "metadata": {
        "id": "WZ1i3v2VtXNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прежде чем работать в Streamlit, нужно создать индекс. Сверху показано как это сделать."
      ],
      "metadata": {
        "id": "dds9sIAZeU8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_streamlit = SearchModel(EmbedderRuCLIP(device='cpu'), DummyIndexer())"
      ],
      "metadata": {
        "id": "CJ3s9-CltdT8"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'/content/stl10/train_images' - Путь к самим изображениям\n",
        "\n",
        "'/content/stl10/features' - Путь, где находится csv и npy файл\n",
        "\n",
        "На сервере в папке датасета нужно будет создать папку \"features\" в ней \"CLIP\"/\"RuCLIP\" или скачивать всё с Colab и переносить в нужный путь\n",
        "\n",
        "Выбор prefix зависит от подаваемого текста или же от того, на какой модели пользователь хочет получить результат, в данный момент поддерживаются \"CLIP\" и \"RuCLIP\"\n"
      ],
      "metadata": {
        "id": "072xH1FiuJaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_streamlit.load_imgs('/content/stl10/train_images','RuCLIP')"
      ],
      "metadata": {
        "id": "CedSpkl3tgpl"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_streamlit.indexer.load(str(test_streamlit.features_path) + '/features.npy')"
      ],
      "metadata": {
        "id": "ZGBskh7atl0t"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = test_streamlit.embedder.encode_text(text=\"Обезьяна играет с мячиком\")\n",
        "test_streamlit.get_k_imgs(query, 15)"
      ],
      "metadata": {
        "id": "0RZ-PitLto8N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}